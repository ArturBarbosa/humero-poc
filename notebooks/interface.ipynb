{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path\n",
    "if \"/Users/artur/dev/humero-poc/\" not in sys.path:\n",
    "    sys.path.append(\"/Users/artur/dev/humero-poc/\")\n",
    "from humero.main import Humero\n",
    "\n",
    "agent = Humero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAdding papers:\u001b[0m\n",
      "   - WikiChat: A Few-Shot LLM-Based Chatbot Grounded with Wikipedia\n",
      "{'paperId': 'c851af5d9959d935dfae5ae5c02283384432b632', 'externalIds': {'ArXiv': '2305.14292', 'DBLP': 'journals/corr/abs-2305-14292', 'DOI': '10.48550/arXiv.2305.14292', 'CorpusId': 258841157}, 'corpusId': 258841157, 'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10', 'name': 'arXiv.org', 'alternate_names': ['ArXiv'], 'issn': '2331-8422', 'url': 'http://bibpurl.oclc.org/web/7130'}, 'url': 'https://www.semanticscholar.org/paper/c851af5d9959d935dfae5ae5c02283384432b632', 'title': 'WikiChat: A Few-Shot LLM-Based Chatbot Grounded with Wikipedia', 'abstract': 'Despite recent advances in Large Language Models (LLMs), users still cannot trust the information provided in their responses. LLMs cannot speak accurately about events that occurred after their training, which are often topics of great interest to users, and, as we show in this paper, they are highly prone to hallucination when talking about less popular (tail) topics. This paper presents WikiChat, a few-shot LLM-based chatbot that is grounded with live information from Wikipedia. Through many iterations of experimentation, we have crafte a pipeline based on information retrieval that (1) uses LLMs to suggest interesting and relevant facts that are individually verified against Wikipedia, (2) retrieves additional up-to-date information, and (3) composes coherent and engaging time-aware responses. We propose a novel hybrid human-and-LLM evaluation methodology to analyze the factuality and conversationality of LLM-based chatbots. We focus on evaluating important but previously neglected issues such as conversing about recent and tail topics. We evaluate WikiChat against strong fine-tuned and LLM-based baselines across a diverse set of conversation topics. We find that WikiChat outperforms all baselines in terms of the factual accuracy of its claims, by up to 12.1%, 28.3% and 32.7% on head, recent and tail topics, while matching GPT-3.5 in terms of providing natural, relevant, non-repetitive and informational responses.', 'venue': 'arXiv.org', 'year': 2023, 'referenceCount': 43, 'citationCount': 0, 'influentialCitationCount': 0, 'isOpenAccess': False, 'openAccessPdf': None, 'fieldsOfStudy': ['Computer Science'], 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}], 'publicationTypes': ['JournalArticle'], 'publicationDate': '2023-05-23', 'journal': {'name': 'ArXiv', 'volume': 'abs/2305.14292'}, 'authors': [{'authorId': '1922611796', 'name': 'Sina J. Semnani'}, {'authorId': '1986608238', 'name': 'Violet Z. Yao'}, {'authorId': '2153528787', 'name': 'He Zhang'}, {'authorId': '39682108', 'name': 'M. Lam'}]}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m paper_list \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marXiv:2305.14292\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marXiv:2307.03172\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marXiv:2304.11556\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m ]\n\u001b[0;32m----> 6\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_paper_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaper_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/humero-poc/humero/main.py:54\u001b[0m, in \u001b[0;36mHumero.add_paper_list\u001b[0;34m(self, paper_ids, level)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   -\u001b[39m\u001b[38;5;124m\"\u001b[39m, paper\u001b[38;5;241m.\u001b[39mtitle)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(paper)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpaper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreferences \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# get references recursively\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# TODO: Some paperIds are None, is it possible to fetch it using other info?\u001b[39;00m\n\u001b[1;32m     58\u001b[0m reference_ids \u001b[38;5;241m=\u001b[39m [reference\u001b[38;5;241m.\u001b[39mpaperId \u001b[38;5;28;01mfor\u001b[39;00m reference \u001b[38;5;129;01min\u001b[39;00m paper\u001b[38;5;241m.\u001b[39mreferences \u001b[38;5;28;01mif\u001b[39;00m reference\u001b[38;5;241m.\u001b[39mpaperId \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "paper_list = [\n",
    "     'arXiv:2305.14292',\n",
    "     'arXiv:2307.03172',\n",
    "     'arXiv:2304.11556'\n",
    "]\n",
    "agent.add_paper_list(paper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSources: \u001b[0m\n",
      "-----------------------------------------\n",
      "\u001b[1mPage: \u001b[0m11\n",
      "\u001b[1mDocument: \u001b[0m/var/folders/br/2vy127pj7974fg9x0rwsp4gm0000gn/T/tmpix8t127c/tmp.pdf\n",
      "-----------------------------------------\n",
      "\u001b[1mPage: \u001b[0m6\n",
      "\u001b[1mDocument: \u001b[0m/var/folders/br/2vy127pj7974fg9x0rwsp4gm0000gn/T/tmpean8opu8/tmp.pdf\n",
      "-----------------------------------------\n",
      "\u001b[1mPage: \u001b[0m4\n",
      "\u001b[1mDocument: \u001b[0m/var/folders/br/2vy127pj7974fg9x0rwsp4gm0000gn/T/tmph4_3__ma/tmp.pdf\n",
      "-----------------------------------------\n",
      "\u001b[1mPage: \u001b[0m2\n",
      "\u001b[1mDocument: \u001b[0m/var/folders/br/2vy127pj7974fg9x0rwsp4gm0000gn/T/tmpean8opu8/tmp.pdf\n",
      "-----------------------------------------\n",
      "\u001b[1mPage: \u001b[0m4\n",
      "\u001b[1mDocument: \u001b[0m/var/folders/br/2vy127pj7974fg9x0rwsp4gm0000gn/T/tmp9w0fwy9j/tmp.pdf\n",
      "-----------------------------------------\n",
      "\u001b[1mPage: \u001b[0m5\n",
      "\u001b[1mDocument: \u001b[0m/var/folders/br/2vy127pj7974fg9x0rwsp4gm0000gn/T/tmp9w0fwy9j/tmp.pdf\n",
      "-----------------------------------------\n",
      "\u001b[1mPage: \u001b[0m4\n",
      "\u001b[1mDocument: \u001b[0m/var/folders/br/2vy127pj7974fg9x0rwsp4gm0000gn/T/tmpix8t127c/tmp.pdf\n",
      "-----------------------------------------\n",
      "\u001b[1mPage: \u001b[0m13\n",
      "\u001b[1mDocument: \u001b[0m/var/folders/br/2vy127pj7974fg9x0rwsp4gm0000gn/T/tmpix8t127c/tmp.pdf\n",
      "-----------------------------------------\n",
      "\u001b[1mPage: \u001b[0m0\n",
      "\u001b[1mDocument: \u001b[0m/var/folders/br/2vy127pj7974fg9x0rwsp4gm0000gn/T/tmp9w0fwy9j/tmp.pdf\n",
      "-----------------------------------------\n",
      "\u001b[1mPage: \u001b[0m1\n",
      "\u001b[1mDocument: \u001b[0m/var/folders/br/2vy127pj7974fg9x0rwsp4gm0000gn/T/tmp9w0fwy9j/tmp.pdf\n",
      "-----------------------------------------\n",
      "\u001b[1mAnswer:\u001b[0mThe Spider dataset is a large-scale human-labeled dataset for complex and cross-domain semantic parsing and the text-to-SQL task. It is used for training and evaluating models in the task of converting natural language questions into SQL queries. The dataset consists of a large number of natural language questions paired with their corresponding SQL queries. It is commonly used in research to develop and evaluate models for tasks such as question answering and database querying.\n"
     ]
    }
   ],
   "source": [
    "agent.ask(\"What is the spider dataset?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
